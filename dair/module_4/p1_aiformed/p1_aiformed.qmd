---
title: "Module 4: AI For Medicine"
author: "Teddy HLA"
format: revealjs
---

## Module 4 Learning Objectives

- Appreciate machine learning  vs. AI
- Understand the high-level steps needed to train a statistical model
- Rationale for 'verifying your model findings' and how to using test set
- understand dataset bias and clinical factors

---

## Scenario 1

- You are a *GP*. 
- You use an AI-based clinical decision support(CDS) tool to help manage patients with ‘breast lump’ presentations.
- The tool helps decide amongst options *‘invasive investigation’*, *‘imaging’* or ‘watchful waiting'. You and colleagues have **good experience** with the tool, your patients have reported satisfactory outcomes.
- A 17-year old male patient presents with ‘breast lump’. 
- Should you *trust* the model recommendations for this patient? 

---

## Scenario 2

- You are a *clinical lead* for your organisation.
- You want to improve pathway for acutely unwell patients. 
- You have **100,000 GBP** budget for this.
- Your team analysed all available options and presented two options.
- Option 1: 'recruiting **two** specialsit nurses'
- Option 2: 'Deployment of **AI-based** clinical decision support and risk management system'

- Which option would you choose?

---

## Big Data, Machine Learning and Artifical Intelligence

- Big data >> ML/AI
- 6 **V** of big data; 'volume', 'variety', 'velocity', 'veracity', 'value' and 'variability'
- I would like to add 'vulnerability'

---

## Machine Learning vs. Artificial Intelligence

* 4 types of machine learning
    + supervised machine learning 
    + unsupervised machine learning
    + Deep Learning (Image recognition)
    + Reinforcement Learning
* Rule of thumb: If you fit a model and it sits on your computer then it is a 'ML' model.
* If this model is deployed and can be used by clinician then it is 'AI'


--- 

## Machine Learning and Philosophy of Learning

* Learning? Why do we learn?
* Difference between learning and memorising
* What does it mean mathematically? 
* What does training mean for you and for a machine learning model? 

---

## Learning vs. Memorising 

* Using an example of 'adding and symbol **+**'
    + You learn to add and you use a symbol **+**
    + So you can *generalise* and you dont need to memorise 1002 + 2003 = 3005 
    + You *train* by doing exercises such as 1+1 = 2 
    + Your teacher corrects if you get 1+1 = 3
    + You will be examined on new material to ensure you can generalise and apply your learnings
* Similarly, you will use a **training data** to train the model so it can *learn* and apply in a **general** context. 

---

## What do learning mean for computer?

$$
Y = f(x)
$$

* Find function f(x) given a set of variables (x) and their outcomes (Y) using a training dataset. 
    + Loss function = similar to making mistakes on training 
    + Optimiser = sort of like teacher, goal is to minimise loss function.

---
## Machine Learning Process

![](mleng.jpg)

---

## Complexity of model

- in ML, you need to be able to trade off between 'bias' and 'variance'
    + very biased model = assuming everyone is sick or assuming 50% is sick 
    + very variant model = responding to every noise.

![](biasvariance.jpg)

---

## Appreciate

![](aimlstats.jpg)

---

![](mleng.jpg)

---

## MLOps : steps in ML

- Get the data, Train the model, Evaluate the model!
- WIN :)
- Getting the data onto your computer
- Train the model on your computer
- Show to your boss, publish!  

**Right??**

---

## MLOps in a nutshell

- Machine Learning + Operations
- MLOps

- ETL pipeline (Extract, Transform and Load) 
- Train the model consecutively (Compute)
- Serve the model to user 

- All in cloud, real-time, continuous deployment

---

## Evaluate your model: how?

- Validation of your model is key.
- Validating on 'training set' = marking your own home work! 
- Thus, you need to do it on a 'test set'.
- Test set is seperate fresh dataset 
- Beware of data leakage

--- 

## What is data leakage?

- Data leakage example in CXR
- CXR for detecting pneumonia? 
- Sicker patients will get AP film and in resus
- So, AI model may learn to 'cheat' by detecting resus instead of actual pneumonia

![](chest.jpg)
---

## Gold standards

<https://www.bmj.com/content/384/bmj-2023-074821>


<https://www.bmj.com/content/384/bmj-2023-074819>

![](extvali.jpg)

---

## Ethics of AI

* AI/ML ethics are complex
    + Should generative AI models such as ChatGPT be allowed when writing research papers?
    + Should you make clinical judgement based on AI recommendations?
* AI/ML dont get it right *but* humans dont get it right either.
    + Does AI/ML need to be better than *humans*? 
    + Which humans? FY1 or average consultant?
* Multiple examples exist

---

## 'Bad' AI

- 'AI gaydar paper' (Wang 2018)

![](aigaydar.jpg)

---

## How do we ~~do good~~ *stop bad* AI

+ Recognise bias and discrimination inherently
+ Assume there will be bias and find it
+ Maximise individual autonomy and privacy
+ Reproducibility and Transparency key

![](turingway.jpg)

--- 

## Even if AI is good?

![](inequality.jpeg){#fig-inequality}

NHS app and deprivation [link to paper](https://informatics.bmj.com/content/30/1/e100809)

---

## Reproducibility: an analogy


::: {#fig-elephants layout-ncol=2}

![](kitchen.jpg){#fig-kitchen}

![](recipe.jpg){#fig-recipe}

Kitchen and Recipe
:::

---

### Reproducibility: questions

- What type of oven?
- What mode of oven ? fan vs. gas
- What kind of oil ? Olive oil vs. rapeseed oil
- What kind of milk? 

- in ML, what python version, which framework etc.

---

## Reproducible analogy

![](repro.jpg)

--- 

## How to make reproducible research

- You wont achieve in your first project
- But, you should aim for it.
- <https://raps-with-r.dev/> 
    + written for R but principles the same

---

## Summary

- ML vs. AI: ML will live on your computer. AI will be usable.
- Extract/Transform/Load data -> Training model(s) -> Serve / Evaluate
- Evaluate your model outputs on external independent test set
- Understand ethics, reproducibility and transparency in your AI modelling

---

## Scenario 1

- You are a *GP*. 
- You use an AI-based clinical decision support(CDS) tool to help manage patients with ‘breast lump’ presentations.
- The tool helps decide amongst options *‘invasive investigation’*, *‘imaging’* or ‘watchful waiting'. You and colleagues have **good experience** with the tool, your patients have reported satisfactory outcomes.
- A 17-year old male patient presents with ‘breast lump’. 
- Should you *trust* the model recommendations for this patient? 

---

### Scenario 1 : What do you think?

- Is the AI CDS trained on male as well as female images?
- Are they trained for <18-year olds?
- Is the model trained to be 'sensitive' or 'specific'?
- What would your clinical judgement be?
- What is the local governance? 

---

## Scenario 2

- You are a *clinical lead* for your organisation.
- You want to improve pathway for acutely unwell patients. 
- You have **100,000 GBP** budget for this.
- Your team analysed all available options and presented two options.
- Option 1: 'recruiting **two** specialist nurses'
- Option 2: 'Deployment of **AI-based** clinical decision support and risk management system'

- Which option would you choose?

---

## Scenario 2: What do you think?

- What is the best option?
- Nurses would be versatile, can help when wards short. 
    + will they in reality? will they be happy? 
    + does that mean they will leave?

- What is our digital debt? Can this cope with AI?
    + Is our pathways aligned with this?
    + if AI said patient's going to deteriorate, what are we going to?
    + But, AI is significantly 'scale-able', doesnt go on annual leave?

## A word on technical debt

- Even google has +++ technical debt
- NHS +++++++

[Technical Debt paper by google](https://research.google.com/pubs/archive/43146.pdf)

